{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a2f97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Available wind scenarios:\n",
      "- simple_static\n",
      "- static_headwind\n",
      "- training_1\n",
      "- training_2\n",
      "- training_3\n"
     ]
    }
   ],
   "source": [
    "# Cellule 1 (au d√©but du notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.my_agent import MyAgent\n",
    "from src.agents.my_agent_DQN import DQNTrainer, MyAgentDQN\n",
    "# Import the evaluation tools\n",
    "from src.test_agent_validity import validate_agent, load_agent_class\n",
    "from src.evaluation import evaluate_agent, visualize_trajectory\n",
    "from wind_scenarios import get_wind_scenario, WIND_SCENARIOS\n",
    "\n",
    "# List available wind scenarios\n",
    "print(\"Available wind scenarios:\")\n",
    "for windfield_name in sorted(WIND_SCENARIOS.keys()):\n",
    "    print(f\"- {windfield_name}\")\n",
    "from typing import Dict, Any\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "from src.utils import save_my_agent\n",
    "from src.utils import save_dqn_agent\n",
    "from src.env_sailing import SailingEnv\n",
    "from src.wind_scenarios import get_wind_scenario\n",
    "from src.agents.my_agent_DQN import collect_normalization_stats, compute_physics_features, compute_physics_features_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961c2768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind scenario: simple_static\n",
      "Using 1 seeds: [1]\n",
      "Max steps per episode: 200\n"
     ]
    }
   ],
   "source": [
    "WIND_SCENARIO_NAME = \"simple_static\" # Options: simple_static, static_headwind, training_1, training_2, training_3, etc.\n",
    "\n",
    "# Evaluation parameters\n",
    "SEEDS = [1]  # Seeds to use for evaluation\n",
    "MAX_HORIZON = 200            # Maximum steps per episode\n",
    "VERBOSE = True               # Show progress bar\n",
    "RENDER = True              # Enable rendering (slower but necessary for visualization)\n",
    "\n",
    "#############################################\n",
    "### DO NOT MODIFY BELOW THIS LINE ##########\n",
    "#############################################\n",
    "\n",
    "\n",
    "print(f\"Wind scenario: {WIND_SCENARIO_NAME}\")\n",
    "print(f\"Using {len(SEEDS)} seeds: {SEEDS}\")\n",
    "print(f\"Max steps per episode: {MAX_HORIZON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_agent(agent=MyAgent, \n",
    "                num_episodes=1000, max_steps = 200, \n",
    "                learning_rate=0.1, discount_factor=0.99, \n",
    "                seed=42, \n",
    "                TRAIN_SCENARIO = 'training_1', \n",
    "                TRAIN_SCENARIOS = ['training_1', 'training_2', 'training_3'],\n",
    "                mu=0.39, prog=9):\n",
    "\n",
    "    ql_agent_full = agent(learning_rate=learning_rate, discount_factor=discount_factor)\n",
    "\n",
    "    # Set fixed seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    ql_agent_full.seed(seed)\n",
    "\n",
    "    \n",
    "\n",
    "    # Create environment with a simple wind scenario\n",
    "    #env = SailingEnv(**get_wind_scenario(TRAIN_SCENARIO))\n",
    "\n",
    "\n",
    "    # Progress tracking\n",
    "    rewards_history = []\n",
    "    steps_history = []\n",
    "    success_history = []\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting full training with 15000 episodes...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    goal = [16, 31]\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment and get initial state\n",
    "        env = SailingEnv(**get_wind_scenario(np.random.choice(TRAIN_SCENARIOS)))\n",
    "        \n",
    "        observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "        state = ql_agent_full.discretize_state(observation)\n",
    "        \n",
    "        total_reward = 0\n",
    "        x_prev, y_prev = observation[0], observation[1]\n",
    "        distance_prev = np.sqrt((goal[0]-x_prev)**2 + (goal[1]-y_prev)**2)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action and take step\n",
    "            action = ql_agent_full.act(observation)\n",
    "            next_observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            x, y = next_observation[0], next_observation[1]\n",
    "            vx, vy = next_observation[2], next_observation[3]\n",
    "\n",
    "            distance_curr = np.sqrt((goal[0]-x)**2 + (goal[1]-y)**2)\n",
    "\n",
    "            progress = distance_prev - distance_curr\n",
    "            progress_reward = prog * progress  \n",
    "\n",
    "            velocity = np.sqrt(vx**2 + vy**2)\n",
    "            velocity_reward = mu * velocity\n",
    "            step_penalty = -0.5\n",
    "\n",
    "            shaped_reward = progress_reward + velocity_reward + reward + step_penalty\n",
    "\n",
    "            next_state = ql_agent_full.discretize_state(next_observation)\n",
    "            \n",
    "            # Update Q-table\n",
    "            ql_agent_full.learn(state, action, shaped_reward, next_state)\n",
    "            \n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            observation = next_observation\n",
    "            total_reward += shaped_reward\n",
    "            distance_prev = distance_curr\n",
    "            \n",
    "            # Break if episode is done\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "           \n",
    "        # Record metrics\n",
    "        rewards_history.append(total_reward)\n",
    "        steps_history.append(step+1) # type: ignore\n",
    "        success_history.append(done) # type: ignore\n",
    "        \n",
    "        # Update exploration rate (decrease over time) we can try UCB\n",
    "        # ql_agent_full.exploration_rate = max(0.05, ql_agent_full.exploration_rate * 0.98)\n",
    "        #ql_agent_full.learning_rate = 0.1 / (1 + episode / 1000)\n",
    "        ql_agent_full.learning_rate = max(0.005, ql_agent_full.learning_rate * 0.998)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Print progress every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            success_rate = sum(success_history[-100:]) / 100 * 100\n",
    "            print(f\"Episode {episode+1}/1000: Success rate (last 100): {success_rate:.1f}%\")\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Calculate overall success rate\n",
    "    success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "    print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "    print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "    print(f\"Q-table size: {len(ql_agent_full.q_table)} states\")\n",
    "    print(f\"\\nFinal Q-table size: {len(ql_agent_full.q_table)} states\")\n",
    "    return ql_agent_full, rewards_history, steps_history, success_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, rewards_history, steps_history, success_history = train_agent(agent=MyAgent, \n",
    "                                                              num_episodes=12000, \n",
    "                                                              max_steps = 200, \n",
    "                                                              learning_rate=0.1, discount_factor=0.99, \n",
    "                                                              seed=42, TRAIN_SCENARIO = 'training_1', \n",
    "                                                              mu=0.39, prog=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate rolling averages\n",
    "window_size = 100\n",
    "rolling_rewards = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_steps = np.convolve(steps_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_success = np.convolve([1 if s else 0 for s in success_history], np.ones(window_size)/window_size, mode='valid') * 100\n",
    "\n",
    "# Create the plots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "#fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(rolling_rewards)\n",
    "ax1.set_ylabel('Average Reward')\n",
    "ax1.set_title('Training Progress (10-episode rolling average)')\n",
    "\n",
    "# Plot steps\n",
    "ax2.plot(rolling_steps)\n",
    "ax2.set_ylabel('Average Steps')\n",
    "\n",
    "# Plot success rate\n",
    "ax3.plot(rolling_success)\n",
    "ax3.set_ylabel('Success Rate (%)')\n",
    "ax3.set_xlabel('Episode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea18a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_results(results):\n",
    "    \"\"\"Print evaluation results in a readable format.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Success Rate: {results['success_rate']:.2%}\")\n",
    "    print(f\"Mean Reward: {results['mean_reward']:.2f} ¬± {results['std_reward']:.2f}\")\n",
    "    print(f\"Mean Steps: {results['mean_steps']:.1f} ¬± {results['std_steps']:.1f}\")\n",
    "    \n",
    "    if 'individual_results' in results:\n",
    "        print(\"\\nIndividual Episode Results:\")\n",
    "        for i, episode in enumerate(results['individual_results']):\n",
    "            print(f\"  Seed {episode['seed']}: \" + \n",
    "                  f\"Reward={episode['reward']:.1f}, \" +\n",
    "                  f\"Steps={episode['steps']}, \" +\n",
    "                  f\"Success={'‚úì' if episode['success'] else '‚úó'}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80597a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### MODIFY THESE PARAMETERS AS NEEDED ######\n",
    "#############################################\n",
    "\n",
    "# Choose which wind scenarios to evaluate on\n",
    "TRAINING_WIND_SCENARIOS = [\"simple_static\", \"training_1\", \"training_2\", \"training_3\"]\n",
    "\n",
    "# Evaluation parameters for all wind scenarios\n",
    "ALL_SEEDS = [42, 43, 44, 45, 46]  # Seeds to use for all evaluations\n",
    "ALL_MAX_HORIZON = 200             # Maximum steps per episode\n",
    "\n",
    "#############################################\n",
    "### DO NOT MODIFY BELOW THIS LINE ##########\n",
    "#############################################\n",
    "\n",
    "\n",
    "all_results = {}\n",
    "    \n",
    "print(f\"Evaluating agent on {len(TRAINING_WIND_SCENARIOS)} wind scenarios (including simple static)...\")\n",
    "    \n",
    "    # Evaluate on each wind scenario\n",
    "for wind_scenario_name in TRAINING_WIND_SCENARIOS:\n",
    "    print(f\"\\nWind scenario: {wind_scenario_name}\")\n",
    "        \n",
    "        # Get the wind scenario\n",
    "    wind_scenario = get_wind_scenario(wind_scenario_name)\n",
    "        \n",
    "        # Run the evaluation\n",
    "    results = evaluate_agent(\n",
    "            agent=agent,\n",
    "            wind_scenario=wind_scenario,\n",
    "            seeds=ALL_SEEDS,\n",
    "            max_horizon=ALL_MAX_HORIZON,\n",
    "            verbose=False,  # Less verbose for multiple evaluations\n",
    "            render=True,\n",
    "            full_trajectory=True\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "    all_results[wind_scenario_name] = results\n",
    "        \n",
    "        # Print summary\n",
    "    print(f\"  Success Rate: {results['success_rate']:.2%}\")\n",
    "    print(f\"  Mean Reward: {results['mean_reward']:.2f}\")\n",
    "    print(f\"  Mean Steps: {results['mean_steps']:.1f}\")\n",
    "    \n",
    "    # Print overall performance\n",
    "total_success = sum(r['success_rate'] for r in all_results.values()) / len(all_results)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"OVERALL SUCCESS RATE: {total_success:.2%}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_my_agent(\n",
    "    agent=agent,\n",
    "    output_path='../src/submission/my_agent.py',\n",
    "    agent_class_name='MyAgent'\n",
    ")\n",
    "\n",
    "print(\"‚úì Agent saved to my_agent.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f1c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b40fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e897cad",
   "metadata": {},
   "source": [
    "## DQN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598d7f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting normalization statistics over 1000 episodes...\n",
      "  Episode 20/1000\n",
      "  Episode 40/1000\n",
      "  Episode 60/1000\n",
      "  Episode 80/1000\n",
      "  Episode 100/1000\n",
      "  Episode 120/1000\n",
      "  Episode 140/1000\n",
      "  Episode 160/1000\n",
      "  Episode 180/1000\n",
      "  Episode 200/1000\n",
      "  Episode 220/1000\n",
      "  Episode 240/1000\n",
      "  Episode 260/1000\n",
      "  Episode 280/1000\n",
      "  Episode 300/1000\n",
      "  Episode 320/1000\n",
      "  Episode 340/1000\n",
      "  Episode 360/1000\n",
      "  Episode 380/1000\n",
      "  Episode 400/1000\n",
      "  Episode 420/1000\n",
      "  Episode 440/1000\n",
      "  Episode 460/1000\n",
      "  Episode 480/1000\n",
      "  Episode 500/1000\n",
      "  Episode 520/1000\n",
      "  Episode 540/1000\n",
      "  Episode 560/1000\n",
      "  Episode 580/1000\n",
      "  Episode 600/1000\n",
      "  Episode 620/1000\n",
      "  Episode 640/1000\n",
      "  Episode 660/1000\n",
      "  Episode 680/1000\n",
      "  Episode 700/1000\n",
      "  Episode 720/1000\n",
      "  Episode 740/1000\n",
      "  Episode 760/1000\n",
      "  Episode 780/1000\n",
      "  Episode 800/1000\n",
      "  Episode 820/1000\n",
      "  Episode 840/1000\n",
      "  Episode 860/1000\n",
      "  Episode 880/1000\n",
      "  Episode 900/1000\n",
      "  Episode 920/1000\n",
      "  Episode 940/1000\n",
      "  Episode 960/1000\n",
      "  Episode 980/1000\n",
      "  Episode 1000/1000\n",
      "‚úì Stats saved to normalization_stats.pkl\n",
      "Feature means: [ 3.0344770e+01  1.5336728e+00  6.4928097e-01  3.0006409e+00\n",
      " -1.4352226e-01 -4.4055212e-01 -2.5167916e-02  3.0738291e+01\n",
      " -3.9398366e-01 -1.3322753e+00  2.9947329e+00 -4.5197472e-04\n",
      "  2.8610569e-01 -4.6641007e-01  3.7271380e-02]\n",
      "Feature stds: [2.9249268  0.2879615  0.40528658 0.22029994 0.65921944 0.5336879\n",
      " 1.293241   3.3061597  0.6777205  1.6052179  0.21716067 0.08825349\n",
      " 0.7938838  0.7201869  0.2786854 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.0344770e+01,  1.5336728e+00,  6.4928097e-01,  3.0006409e+00,\n",
       "        -1.4352226e-01, -4.4055212e-01, -2.5167916e-02,  3.0738291e+01,\n",
       "        -3.9398366e-01, -1.3322753e+00,  2.9947329e+00, -4.5197472e-04,\n",
       "         2.8610569e-01, -4.6641007e-01,  3.7271380e-02], dtype=float32),\n",
       " array([2.9249268 , 0.2879615 , 0.40528658, 0.22029994, 0.65921944,\n",
       "        0.5336879 , 1.293241  , 3.3061597 , 0.6777205 , 1.6052179 ,\n",
       "        0.21716067, 0.08825349, 0.7938838 , 0.7201869 , 0.2786854 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SailingEnv(**get_wind_scenario('training_1'))\n",
    "    \n",
    "# 2. Collecter les stats de normalisation (UNE FOIS)\n",
    "collect_normalization_stats(env, n_episodes=1000, save_path='normalization_stats.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737445bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SailingEnv(**get_wind_scenario('training_1')) # type: ignore\n",
    "    \n",
    "# Cr√©er le trainer\n",
    "trainer = DQNTrainer(\n",
    "        env,\n",
    "        stats_path='normalization_stats.pkl',\n",
    "        learning_rate=3e-4,\n",
    "        lr_decay=0.9999,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.05,\n",
    "        epsilon_decay=0.9995,\n",
    "        buffer_capacity=100000,\n",
    "        target_update_freq=50,\n",
    "        device='cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8cc9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target network updated at step 100\n",
      "Target network updated at step 200\n",
      "Episode 0/200 | Reward: 153.95 | Epsilon: 0.905 | LR: 0.000299 | Buffer: 200 | Steps: 200\n",
      "Target network updated at step 300\n",
      "Target network updated at step 400\n",
      "Target network updated at step 500\n",
      "Target network updated at step 600\n",
      "Target network updated at step 700\n",
      "Target network updated at step 800\n",
      "Target network updated at step 900\n",
      "Target network updated at step 1000\n",
      "Target network updated at step 1100\n",
      "Target network updated at step 1200\n",
      "Target network updated at step 1300\n",
      "Target network updated at step 1400\n",
      "Target network updated at step 1500\n",
      "Target network updated at step 1600\n",
      "Target network updated at step 1700\n",
      "Target network updated at step 1800\n",
      "Target network updated at step 1900\n",
      "Target network updated at step 2000\n",
      "Target network updated at step 2100\n",
      "Episode 10/200 | Reward: 76.13 | Epsilon: 0.341 | LR: 0.000285 | Buffer: 2154 | Steps: 2154\n",
      "Target network updated at step 2200\n",
      "Target network updated at step 2300\n",
      "Target network updated at step 2400\n",
      "Target network updated at step 2500\n",
      "Target network updated at step 2600\n",
      "Target network updated at step 2700\n",
      "Target network updated at step 2800\n",
      "Target network updated at step 2900\n",
      "Target network updated at step 3000\n",
      "Target network updated at step 3100\n",
      "Target network updated at step 3200\n",
      "Target network updated at step 3300\n",
      "Target network updated at step 3400\n",
      "Target network updated at step 3500\n",
      "Target network updated at step 3600\n",
      "Target network updated at step 3700\n",
      "Target network updated at step 3800\n",
      "Episode 20/200 | Reward: 364.59 | Epsilon: 0.148 | LR: 0.000273 | Buffer: 3826 | Steps: 3826\n",
      "Target network updated at step 3900\n",
      "Target network updated at step 4000\n",
      "Target network updated at step 4100\n",
      "Target network updated at step 4200\n",
      "Target network updated at step 4300\n",
      "Target network updated at step 4400\n",
      "Target network updated at step 4500\n",
      "Target network updated at step 4600\n",
      "Target network updated at step 4700\n",
      "Target network updated at step 4800\n",
      "Target network updated at step 4900\n",
      "Episode 30/200 | Reward: 367.21 | Epsilon: 0.082 | LR: 0.000265 | Buffer: 4991 | Steps: 4991\n",
      "Target network updated at step 5000\n",
      "Target network updated at step 5100\n",
      "Target network updated at step 5200\n",
      "Target network updated at step 5300\n",
      "Target network updated at step 5400\n",
      "Target network updated at step 5500\n",
      "Target network updated at step 5600\n",
      "Target network updated at step 5700\n",
      "Target network updated at step 5800\n",
      "Target network updated at step 5900\n",
      "Target network updated at step 6000\n",
      "Target network updated at step 6100\n",
      "Episode 40/200 | Reward: 361.63 | Epsilon: 0.050 | LR: 0.000258 | Buffer: 6107 | Steps: 6107\n",
      "Target network updated at step 6200\n",
      "Target network updated at step 6300\n",
      "Target network updated at step 6400\n",
      "Target network updated at step 6500\n",
      "Target network updated at step 6600\n",
      "Target network updated at step 6700\n",
      "Target network updated at step 6800\n",
      "Target network updated at step 6900\n",
      "Target network updated at step 7000\n",
      "Target network updated at step 7100\n",
      "Episode 50/200 | Reward: 346.63 | Epsilon: 0.050 | LR: 0.000251 | Buffer: 7172 | Steps: 7172\n",
      "[EVAL] Episode 50 | Eval Reward: 100.00 | Success Rate: 100.0%\n",
      "‚úì New best model saved! (Reward: 100.00)\n",
      "Target network updated at step 7200\n",
      "Target network updated at step 7300\n",
      "Target network updated at step 7400\n",
      "Target network updated at step 7500\n",
      "Target network updated at step 7600\n",
      "Target network updated at step 7700\n",
      "Target network updated at step 7800\n",
      "Target network updated at step 7900\n",
      "Target network updated at step 8000\n",
      "Target network updated at step 8100\n",
      "Target network updated at step 8200\n",
      "Target network updated at step 8300\n",
      "Target network updated at step 8400\n",
      "Episode 60/200 | Reward: 344.00 | Epsilon: 0.050 | LR: 0.000243 | Buffer: 8486 | Steps: 8486\n",
      "Target network updated at step 8500\n",
      "Target network updated at step 8600\n",
      "Target network updated at step 8700\n",
      "Target network updated at step 8800\n",
      "Target network updated at step 8900\n",
      "Target network updated at step 9000\n",
      "Target network updated at step 9100\n",
      "Target network updated at step 9200\n",
      "Episode 70/200 | Reward: 362.91 | Epsilon: 0.050 | LR: 0.000238 | Buffer: 9282 | Steps: 9282\n",
      "Target network updated at step 9300\n",
      "Target network updated at step 9400\n",
      "Target network updated at step 9500\n",
      "Target network updated at step 9600\n",
      "Target network updated at step 9700\n",
      "Target network updated at step 9800\n",
      "Target network updated at step 9900\n",
      "Target network updated at step 10000\n",
      "Target network updated at step 10100\n",
      "Target network updated at step 10200\n",
      "Target network updated at step 10300\n",
      "Target network updated at step 10400\n",
      "Episode 80/200 | Reward: 313.86 | Epsilon: 0.050 | LR: 0.000231 | Buffer: 10472 | Steps: 10472\n",
      "Target network updated at step 10500\n",
      "Target network updated at step 10600\n",
      "Target network updated at step 10700\n",
      "Target network updated at step 10800\n",
      "Target network updated at step 10900\n",
      "Target network updated at step 11000\n",
      "Target network updated at step 11100\n",
      "Target network updated at step 11200\n",
      "Target network updated at step 11300\n",
      "Target network updated at step 11400\n",
      "Target network updated at step 11500\n",
      "Target network updated at step 11600\n",
      "Episode 90/200 | Reward: 364.45 | Epsilon: 0.050 | LR: 0.000225 | Buffer: 11631 | Steps: 11631\n",
      "Target network updated at step 11700\n",
      "Target network updated at step 11800\n",
      "Target network updated at step 11900\n",
      "Target network updated at step 12000\n",
      "Target network updated at step 12100\n",
      "Target network updated at step 12200\n",
      "Target network updated at step 12300\n",
      "Target network updated at step 12400\n",
      "Target network updated at step 12500\n",
      "Target network updated at step 12600\n",
      "Target network updated at step 12700\n",
      "Target network updated at step 12800\n",
      "Target network updated at step 12900\n",
      "Episode 100/200 | Reward: 337.45 | Epsilon: 0.050 | LR: 0.000218 | Buffer: 12915 | Steps: 12915\n",
      "[EVAL] Episode 100 | Eval Reward: 100.00 | Success Rate: 100.0%\n",
      "Target network updated at step 13000\n",
      "Target network updated at step 13100\n",
      "Target network updated at step 13200\n",
      "Target network updated at step 13300\n",
      "Target network updated at step 13400\n",
      "Target network updated at step 13500\n",
      "Target network updated at step 13600\n",
      "Target network updated at step 13700\n",
      "Target network updated at step 13800\n",
      "Target network updated at step 13900\n",
      "Target network updated at step 14000\n",
      "Episode 110/200 | Reward: 366.54 | Epsilon: 0.050 | LR: 0.000212 | Buffer: 14017 | Steps: 14017\n",
      "Target network updated at step 14100\n",
      "Target network updated at step 14200\n",
      "Target network updated at step 14300\n",
      "Target network updated at step 14400\n",
      "Target network updated at step 14500\n",
      "Target network updated at step 14600\n",
      "Target network updated at step 14700\n",
      "Target network updated at step 14800\n",
      "Target network updated at step 14900\n",
      "Target network updated at step 15000\n",
      "Target network updated at step 15100\n",
      "Target network updated at step 15200\n",
      "Target network updated at step 15300\n",
      "Target network updated at step 15400\n",
      "Episode 120/200 | Reward: 132.06 | Epsilon: 0.050 | LR: 0.000204 | Buffer: 15466 | Steps: 15466\n",
      "Target network updated at step 15500\n",
      "Target network updated at step 15600\n",
      "Target network updated at step 15700\n",
      "Target network updated at step 15800\n",
      "Target network updated at step 15900\n",
      "Target network updated at step 16000\n",
      "Target network updated at step 16100\n",
      "Target network updated at step 16200\n",
      "Target network updated at step 16300\n",
      "Target network updated at step 16400\n",
      "Target network updated at step 16500\n",
      "Target network updated at step 16600\n",
      "Target network updated at step 16700\n",
      "Episode 130/200 | Reward: 311.27 | Epsilon: 0.050 | LR: 0.000198 | Buffer: 16750 | Steps: 16750\n",
      "Target network updated at step 16800\n",
      "Target network updated at step 16900\n",
      "Target network updated at step 17000\n",
      "Target network updated at step 17100\n",
      "Target network updated at step 17200\n",
      "Target network updated at step 17300\n",
      "Target network updated at step 17400\n",
      "Target network updated at step 17500\n",
      "Target network updated at step 17600\n",
      "Target network updated at step 17700\n",
      "Target network updated at step 17800\n",
      "Target network updated at step 17900\n",
      "Target network updated at step 18000\n",
      "Episode 140/200 | Reward: 302.88 | Epsilon: 0.050 | LR: 0.000191 | Buffer: 18072 | Steps: 18072\n",
      "Target network updated at step 18100\n",
      "Target network updated at step 18200\n",
      "Target network updated at step 18300\n",
      "Target network updated at step 18400\n",
      "Target network updated at step 18500\n",
      "Target network updated at step 18600\n",
      "Target network updated at step 18700\n",
      "Target network updated at step 18800\n",
      "Target network updated at step 18900\n",
      "Target network updated at step 19000\n",
      "Target network updated at step 19100\n",
      "Target network updated at step 19200\n",
      "Target network updated at step 19300\n",
      "Target network updated at step 19400\n",
      "Target network updated at step 19500\n",
      "Episode 150/200 | Reward: 348.48 | Epsilon: 0.050 | LR: 0.000184 | Buffer: 19518 | Steps: 19518\n",
      "[EVAL] Episode 150 | Eval Reward: 100.00 | Success Rate: 100.0%\n",
      "Target network updated at step 19600\n",
      "Target network updated at step 19700\n",
      "Target network updated at step 19800\n",
      "Target network updated at step 19900\n",
      "Target network updated at step 20000\n",
      "Target network updated at step 20100\n",
      "Target network updated at step 20200\n",
      "Target network updated at step 20300\n",
      "Target network updated at step 20400\n",
      "Target network updated at step 20500\n",
      "Episode 160/200 | Reward: 362.92 | Epsilon: 0.050 | LR: 0.000180 | Buffer: 20576 | Steps: 20576\n",
      "Target network updated at step 20600\n",
      "Target network updated at step 20700\n",
      "Target network updated at step 20800\n",
      "Target network updated at step 20900\n",
      "Target network updated at step 21000\n",
      "Target network updated at step 21100\n",
      "Target network updated at step 21200\n",
      "Target network updated at step 21300\n",
      "Target network updated at step 21400\n",
      "Target network updated at step 21500\n",
      "Target network updated at step 21600\n",
      "Episode 170/200 | Reward: 312.84 | Epsilon: 0.050 | LR: 0.000175 | Buffer: 21624 | Steps: 21624\n",
      "Target network updated at step 21700\n",
      "Target network updated at step 21800\n",
      "Target network updated at step 21900\n",
      "Target network updated at step 22000\n",
      "Target network updated at step 22100\n",
      "Target network updated at step 22200\n",
      "Target network updated at step 22300\n",
      "Target network updated at step 22400\n",
      "Episode 180/200 | Reward: 328.58 | Epsilon: 0.050 | LR: 0.000171 | Buffer: 22469 | Steps: 22469\n",
      "Target network updated at step 22500\n",
      "Target network updated at step 22600\n",
      "Target network updated at step 22700\n",
      "Target network updated at step 22800\n",
      "Target network updated at step 22900\n",
      "Target network updated at step 23000\n",
      "Target network updated at step 23100\n",
      "Target network updated at step 23200\n",
      "Target network updated at step 23300\n",
      "Target network updated at step 23400\n",
      "Target network updated at step 23500\n",
      "Target network updated at step 23600\n",
      "Target network updated at step 23700\n",
      "Episode 190/200 | Reward: 117.14 | Epsilon: 0.050 | LR: 0.000166 | Buffer: 23782 | Steps: 23782\n",
      "Target network updated at step 23800\n",
      "Target network updated at step 23900\n",
      "Target network updated at step 24000\n",
      "Target network updated at step 24100\n",
      "Target network updated at step 24200\n",
      "Target network updated at step 24300\n",
      "Target network updated at step 24400\n",
      "Target network updated at step 24500\n"
     ]
    }
   ],
   "source": [
    "# Entra√Æner\n",
    "trainer.train(num_episodes=200, eval_freq=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ad0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16425283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d997cd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING DQN AGENT FOR SUBMISSION\n",
      "======================================================================\n",
      "\n",
      "üìä Agent Statistics:\n",
      "   Network parameters: 128,921\n",
      "   Physics features: 15\n",
      "   Goal position: (16, 31)\n",
      "\n",
      "üíæ Embedding network weights...\n",
      "   5/18 layers embedded\n",
      "   10/18 layers embedded\n",
      "   15/18 layers embedded\n",
      "   ‚úì All 18 layers embedded\n",
      "\n",
      "üìù Writing to submission/my_agent.py...\n",
      "\n",
      "======================================================================\n",
      "‚úÖ AGENT SAVED SUCCESSFULLY\n",
      "======================================================================\n",
      "\n",
      "üìÑ Output file: submission/my_agent.py\n",
      "üìä File size: 1.93 MB\n",
      "üß† Network parameters: 128,921\n",
      "üéØ Physics features: 15\n",
      "\n",
      "üìã Next steps:\n",
      "   1. Validate: python src/test_agent_validity.py submission/my_agent.py\n",
      "   2. Evaluate: python src/evaluate_submission.py submission/my_agent.py --num-seeds 10\n",
      "   3. Submit: Upload submission/my_agent.py to competition platform\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_dqn_agent(trainer, 'submission/my_agent.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
