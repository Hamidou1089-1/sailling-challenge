# Q-Learning Baseline Configuration
# Based on your successful experiments

# Training parameters
training:
  num_episodes: 12000
  max_steps_per_episode: 200
  eval_freq: 1000
  save_freq: 2000
  
  # Wind scenarios
  train_scenarios: ['training_1', 'training_2', 'training_3']
  eval_scenarios: ['training_1', 'training_2', 'training_3']
  
  seed: 42

# Agent hyperparameters
agent:
  # Learning rate (with decay)
  learning_rate: 0.1
  lr_decay: 0.998
  min_lr: 0.005
  
  # Exploration (UCB-based, no epsilon)
  use_ucb: true
  ucb_c: 2.0              # Exploration constant for UCB
  
  # Discount factor
  gamma: 0.99
  
  # Reward shaping
  progress_reward_coeff: 9.0      # mu
  velocity_reward_coeff: 0.39     # nu
  step_penalty: -0.5

# Environment
environment:
  goal: [16, 31]
  
# State discretization
discretization:
  distance_bins: 12       # Fine-grained distance bins
  angle_bins: 12
  wind_ahead_depth: 3     # Look-ahead depth for wind
  wind_asymmetry_depth: 3

# Checkpointing
checkpoint:
  save_dir: "checkpoints/qlearning"
  resume_from: null

# Logging
logging:
  tensorboard_dir: "runs/qlearning"
  experiment_name: "qlearning_ucb_baseline"